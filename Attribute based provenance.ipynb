{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provenance Tensor for Vertical Reduction:\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "\n",
      "Attribute Provenance for Vertical Reduction:\n",
      "[1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Provenance:\n",
    "    def __init__(self):\n",
    "        self.provenance_data = {}\n",
    "        self.attribute_provenance = {}\n",
    "\n",
    "    def capture_vertical_reduction(self, din, dout, retained_columns):\n",
    "        n = len(din)\n",
    "        m = len(dout)\n",
    "\n",
    "        # Capture the bitset for retained columns\n",
    "        bitset = [1 if col in retained_columns else 0 for col in din.columns]\n",
    "\n",
    "        # Create the provenance tensor (identity matrix)\n",
    "        T = np.eye(m, n, dtype=int)\n",
    "\n",
    "        # Store the provenance tensor and attribute-based provenance\n",
    "        self.provenance_data[\"vertical_reduction\"] = T\n",
    "        self.attribute_provenance[\"vertical_reduction\"] = bitset\n",
    "\n",
    "        return T\n",
    "\n",
    "\n",
    "\n",
    "    def get_provenance(self, operation_key):\n",
    "        return self.provenance_data.get(operation_key, None)\n",
    "\n",
    "    def get_attribute_provenance(self, operation_key):\n",
    "        return self.attribute_provenance.get(operation_key, None)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.provenance_data)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Creating the Provenance instance\n",
    "prov = Provenance()\n",
    "\n",
    "# Input DataFrame\n",
    "df_din = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "})\n",
    "\n",
    "# Output DataFrame after vertical reduction (removing column 'B')\n",
    "df_dout = df_din[['A', 'C']]\n",
    "\n",
    "# Capture the provenance for vertical reduction\n",
    "prov.capture_vertical_reduction(df_din, df_dout, retained_columns=['A', 'C'])\n",
    "\n",
    "# Retrieve and print the provenance tensor and attribute provenance\n",
    "print(\"Provenance Tensor for Vertical Reduction:\")\n",
    "print(prov.get_provenance(\"vertical_reduction\"))\n",
    "\n",
    "print(\"\\nAttribute Provenance for Vertical Reduction:\")\n",
    "print(prov.get_attribute_provenance(\"vertical_reduction\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Provenance:\n",
    "    def __init__(self):\n",
    "        self.provenance_data = {}\n",
    "\n",
    "    def capture_vertical_augmentation(self, din, dout):\n",
    "\n",
    "        # Number of columns in dout\n",
    "        num_columns_dout = len(dout.columns)\n",
    "\n",
    "        # Initialize the output bitset with zeros\n",
    "        output_bitset = np.zeros(num_columns_dout, dtype=int)\n",
    "\n",
    "        # Identify new columns in dout that are not in din\n",
    "        new_columns = dout.columns.difference(din.columns)\n",
    "\n",
    "        # Set the corresponding bits in the output bitset to 1 for new columns\n",
    "        for new_col in new_columns:\n",
    "            output_index = dout.columns.get_loc(new_col)\n",
    "            output_bitset[output_index] = 1\n",
    "\n",
    "        num_records_din = len(din)\n",
    "        num_records_dout = len(dout)\n",
    "\n",
    "        provenance_tensor = np.eye(num_records_dout, num_records_din, dtype=int)\n",
    "\n",
    "        # Store the provenance information\n",
    "        self.provenance_data['vertical_augmentation'] = {\n",
    "            'record_tensor': provenance_tensor,\n",
    "            'output_bitset': output_bitset\n",
    "        }\n",
    "\n",
    "        return provenance_tensor, output_bitset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provenance Tensor:\n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Bitset for New Columns: [0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "din = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "})\n",
    "\n",
    "# Perform vertical augmentation on din to create new columns\n",
    "dout = din.copy()\n",
    "dout['D'] = din['A'] + din['C']  # New column 'D' is created as the sum of columns 'A' and 'C'\n",
    "dout['E'] = din['A'] + din['C'] + din['B']  # New column 'E' is created as the sum of columns 'A', 'C', and 'B'\n",
    "\n",
    "prov = Provenance()\n",
    "provenance_tensor, new_column_bitset = prov.capture_vertical_augmentation(din, dout)\n",
    "\n",
    "print(\"Provenance Tensor:\\n\", provenance_tensor)\n",
    "print(\"Bitset for New Columns:\", new_column_bitset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Provenance:\n",
    "    def __init__(self):\n",
    "        self.provenance_data = {}  # Ensure the provenance_data dictionary is initialized\n",
    "\n",
    "    def hash_row(self, row):\n",
    "        \"\"\"Generate a unique hash for a row.\"\"\"\n",
    "        return hash(tuple(row))\n",
    "\n",
    "    def generate_hashed_df(self, df):\n",
    "        \"\"\"Generate a DataFrame with unique hashes for each row.\"\"\"\n",
    "        hashed_df = df.copy()\n",
    "        hashed_df['hash'] = df.apply(self.hash_row, axis=1)\n",
    "        return hashed_df\n",
    "\n",
    "    def capture_join(self, dl, dr, do, join_condition):\n",
    "        # Step 1: Hash the input DataFrames and the result DataFrame\n",
    "        df_left_hashed = self.generate_hashed_df(dl)\n",
    "        df_right_hashed = self.generate_hashed_df(dr)\n",
    "        df_result_hashed = self.generate_hashed_df(do)\n",
    "\n",
    "        # Step 2: Project the result DataFrame onto the columns of the input DataFrames\n",
    "        df_left_proj = df_result_hashed[dl.columns].copy()\n",
    "        df_right_proj = df_result_hashed[dr.columns].copy()\n",
    "\n",
    "        # Step 3: Generate hashes for the projections\n",
    "        df_left_proj['hash_proj'] = df_left_proj.apply(self.hash_row, axis=1)\n",
    "        df_right_proj['hash_proj'] = df_right_proj.apply(self.hash_row, axis=1)\n",
    "\n",
    "        # Step 4: Map the hashed projections to their indices\n",
    "        n_result = len(do)\n",
    "        n_left = len(dl)\n",
    "        n_right = len(dr)\n",
    "        tensor_prov = np.zeros((n_result, n_left, n_right), dtype=int)\n",
    "\n",
    "        # Using pandas to match indices\n",
    "        left_index_map = df_left_hashed.set_index('hash').index\n",
    "        right_index_map = df_right_hashed.set_index('hash').index\n",
    "\n",
    "        left_indices = df_left_proj['hash_proj'].map(left_index_map.get_loc)\n",
    "        right_indices = df_right_proj['hash_proj'].map(right_index_map.get_loc)\n",
    "\n",
    "        # Fill the provenance tensor\n",
    "        tensor_prov[np.arange(n_result), left_indices, right_indices] = 1\n",
    "\n",
    "        # Automatically generate bitsets based on column names\n",
    "        num_attributes_do = len(do.columns)\n",
    "        bitset_dl = np.zeros(num_attributes_do, dtype=int)\n",
    "        bitset_dr = np.zeros(num_attributes_do, dtype=int)\n",
    "\n",
    "        for i, col in enumerate(do.columns):\n",
    "            if col in dl.columns:\n",
    "                bitset_dl[i] = 1\n",
    "            if col in dr.columns:\n",
    "                bitset_dr[i] = 1\n",
    "\n",
    "        # Store the provenance data\n",
    "        self.provenance_data['join'] = {\n",
    "            'record_tensor': tensor_prov,\n",
    "            'bitset_dl': bitset_dl,\n",
    "            'bitset_dr': bitset_dr\n",
    "        }\n",
    "\n",
    "        return tensor_prov, bitset_dl, bitset_dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provenance Tensor:\n",
      "[[[0 0 0]\n",
      "  [1 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 1 0]]]\n",
      "\n",
      "Bitset for Left DataFrame:\n",
      "[1 1 0]\n",
      "\n",
      "Bitset for Right DataFrame:\n",
      "[1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "df_left = pd.DataFrame({\n",
    "    'ID': [1, 2, 3],\n",
    "    'Value_Left': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "df_right = pd.DataFrame({\n",
    "    'ID': [2, 3, 4],\n",
    "    'Value_Right': ['D', 'E', 'F']\n",
    "})\n",
    "\n",
    "df_result = pd.merge(df_left, df_right, on='ID')\n",
    "\n",
    "# Instantiate the Provenance object\n",
    "prov = Provenance()\n",
    "\n",
    "# Capture the join provenance\n",
    "tensor_prov, bitset_dl, bitset_dr = prov.capture_join(df_left, df_right, df_result, 'ID')\n",
    "\n",
    "# Display the results\n",
    "print(\"Provenance Tensor:\")\n",
    "print(tensor_prov)\n",
    "print(\"\\nBitset for Left DataFrame:\")\n",
    "print(bitset_dl)\n",
    "print(\"\\nBitset for Right DataFrame:\")\n",
    "print(bitset_dr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
